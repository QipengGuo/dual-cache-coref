# @package _global_

# This configuration trains a joint model for Ontonotes, Litbank, and Preco.
# Note that OntoNotes and Preco are downsampled in this configuration.
# OntoNotes also uses pseudo-singletons

# Model name in CRAC 2021: longdoc^S Joint + PS 30K


defaults:
  - override /datasets: joint
  - override /trainer: train.yaml
  - override /model: model.yaml

trainer:
  log_frequency: 500
  max_evals: 20
  eval_per_k_steps: 5000
  patience: 10


datasets:
  ontonotes:
    num_train_docs: 1000
    singleton_file: ontonotes/ment_singletons_longformer_speaker/30.jsonlines

  preco:
    num_train_docs: 1000

model:
  doc_encoder:
    add_speaker_tokens: True

